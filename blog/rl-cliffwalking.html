<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Solving the Cliff Walking Problem | DevLog</title>
    <link rel="icon" type="image/svg+xml"
        href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ðŸ§—</text></svg>">

    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <link rel="stylesheet" href="../assets/css/style.css?v=1.1">
    <script src="../assets/js/tailwind.config.js"></script>
    <script src="../assets/js/theme-toggle.js"></script>
</head>

<body class="bg-slate-900 text-slate-200 antialiased selection:bg-red-500 selection:text-white">

    <div class="fixed inset-0 tech-grid opacity-20 pointer-events-none z-0"></div>

    <!-- Nav -->
    <nav class="fixed w-full z-50 bg-slate-900/80 backdrop-blur-md border-b border-slate-800">
        <div class="max-w-4xl mx-auto px-6 h-16 flex items-center justify-between">
            <a href="../log.html"
                class="font-mono text-lg font-bold text-slate-200 hover:text-purple-400 flex items-center gap-2 transition-colors">
                &lt;Hugo/Logs&gt;
            </a>
        </div>
    </nav>

    <article class="pt-32 pb-24 px-6 max-w-3xl mx-auto relative z-10">
        <!-- Header -->
        <header class="mb-12 border-b border-slate-800 pb-8">
            <div class="flex gap-3 text-sm font-mono text-red-500 mb-4">
                <span>Dec 18, 2025</span>
                <span>â€¢</span>
                <span>Entry #006</span>
            </div>
            <h1 class="text-4xl md:text-5xl font-bold text-white mb-6 leading-tight">
                Solving the Cliff Walking Problem: RL Algorithms Comparison ðŸ§—
            </h1>
            <p class="text-xl text-slate-400 leading-relaxed">
                A comparative study of Q-Learning, SARSA, and Monte Carlo methods in the classic "Cliff Walking"
                environment.
            </p>
        </header>

        <!-- Content -->
        <div
            class="prose prose-invert prose-lg max-w-none prose-headings:font-bold prose-headings:text-white prose-a:text-red-400">

            <h3>The Challenge</h3>
            <p>
                The <strong>Cliff Walking</strong> environment (from Gymnasium) is a grid world where the agent must
                start at separate points (Start and Goal) while avoiding a cliff that results in an instant reset.
            </p>
            <p>
                I implemented and compared three fundamental Reinforcement Learning algorithms to solve this stochastic
                problem.
            </p>

            <h3>The Algorithms</h3>
            <ul>
                <li><strong>Q-Learning (Off-Policy):</strong> Learns the optimal policy directly. It tends to take the
                    riskiest (shortest) path right along the cliff edge because it assumes optimal action selection.
                </li>
                <li><strong>SARSA (On-Policy):</strong> Learns the "safe" path. Since it accounts for the Îµ-greedy
                    policy (random exploration), it keeps a buffer zone away from the cliff to avoid falling due to
                    random moves.</li>
                <li><strong>Monte Carlo (First-Visit):</strong> Learns from complete episodes. Good convergence but high
                    variance in this specific environment.</li>
            </ul>

            <div class="bg-slate-800 p-6 rounded-xl border border-slate-700 my-8">
                <h4 class="text-white font-mono text-sm mb-2 border-b border-slate-700 pb-2">$/ src/algorithms.py</h4>
                <pre class="font-mono text-sm text-slate-300 overflow-x-auto">
def update_q_learning(self, state, action, reward, next_state):
    # Q(s,a) = Q(s,a) + alpha * (R + gamma * max(Q(s',a')) - Q(s,a))
    best_next_action = np.argmax(self.q_table[next_state])
    td_target = reward + self.gamma * self.q_table[next_state][best_next_action]
    self.q_table[state][action] += self.alpha * (td_target - self.q_table[state][action])</pre>
            </div>

            <h3>Results</h3>
            <p>
                The results matched the theory perfectly. <strong>SARSA</strong> learned a safer path (higher average
                reward during training), while <strong>Q-Learning</strong> learned the optimal path but fell off the
                cliff more often during the exploration phase.
            </p>
            <p>
                Check out the full comparison and code in the repository.
            </p>
            <p>
                <a href="https://github.com/Eugegeuge/ReinforcementLearning-CliffWalking" target="_blank"
                    class="inline-flex items-center gap-2 px-4 py-2 bg-slate-800 text-white rounded-lg border border-slate-700 hover:border-red-500 transition-all font-mono text-sm no-underline">
                    <i data-lucide="github" class="w-4 h-4"></i> View Repository
                </a>
            </p>

        </div>
    </article>

    <script> lucide.createIcons(); </script>
</body>

</html>